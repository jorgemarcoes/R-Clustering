{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgemarcoes/R-Clustering/blob/main/R_Clustering_on_UCR_Archive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIVk2IIYZey"
      },
      "source": [
        "#LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z0QW1ScqC4Tq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aeon #comment if aeon library already installed\n",
        "from aeon.datasets import load_classification\n"
      ],
      "metadata": {
        "id": "c-CXPCFFjeZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AQmmDvWLjeHD"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UCR ARCHIVE"
      ],
      "metadata": {
        "id": "lWBiTZ2RzI_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "PS0tW3RrisfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_df = pd.read_csv('https://www.cs.ucr.edu/~eamonn/time_series_data_2018/DataSummary.csv')\n",
        "datasets128 = datasets_df['Name'].values\n",
        "\n",
        "#developmentes datasets\n",
        "development_datasets = ['Beef', 'BirdChicken', 'Car', 'CricketX', 'CricketY', 'CricketZ', 'DistalPhalanxTW',\n",
        "                        'ECG200', 'ECG5000', 'FiftyWords', 'Fish', 'FordA', 'FordB', 'Haptics', 'Herring',\n",
        "                        'InsectWingbeatSound', 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2',\n",
        "                        'Lightning7', 'Meat', 'MedicalImages', 'MiddlePhalanxOutlineAgeGroup', 'OSULeaf',\n",
        "                        'OliveOil', 'Phoneme', 'Plane', 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW',\n",
        "                        'ScreenType', 'ShapeletSim', 'Strawberry', 'SwedishLeaf', 'SyntheticControl',\n",
        "                        'ToeSegmentation1', 'Trace', 'UWaveGestureLibraryY', 'Wafer', 'WordSynonyms',\n",
        "                        'Worms', 'Yoga']\n",
        "\n",
        "\n",
        "varying_length_datasets = ['PLAID', 'AllGestureWiimoteX', 'AllGestureWiimoteY',\n",
        "                   'AllGestureWiimoteZ', 'GestureMidAirD1', 'GestureMidAirD2',\n",
        "                    'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2',\n",
        "                    'PickupGestureWiimoteZ', 'ShakeGestureWiimoteZ']\n",
        "\n",
        "\n",
        "\n",
        "#difference between datasets with setdiff1d\n",
        "validation_datasets = np.setdiff1d(datasets128, development_datasets)\n",
        "validation_datasets\n",
        "\n",
        "#remove varying length datasets from validation datasets\n",
        "validation_datasets = np.setdiff1d(validation_datasets, varying_length_datasets)\n",
        "\n",
        "#Total num of valid validation datasets and label:\n",
        "print('num of valid validation datasets: ', len(validation_datasets))\n",
        "\n",
        "#Total num of valid validation datasets and developement datasets:\n",
        "print('num of valid develompent datasets: ', len(development_datasets))\n",
        "\n",
        "#all valid datasets (datasets128 except varying length datasets)\n",
        "all_valid_datasets = np.setdiff1d(datasets128, varying_length_datasets)\n",
        "print('num of all valid datasets: ', len(all_valid_datasets))\n",
        "\n",
        "\n",
        "#Datasetes with missing values\n",
        "datasets_with_nans = ['DodgerLoopDay', 'DodgerLoopGame', 'DodgerLoopWeekend', 'MelbournePedestrian']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AsfiXENi7zC",
        "outputId": "3ecae315-1a18-4f87-f58a-bc091b59458f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of valid validation datasets:  76\n",
            "num of valid develompent datasets:  41\n",
            "num of all valid datasets:  117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_MelbournePedestrian(X, y):\n",
        "    \"\"\"\n",
        "    Filters out elements from X and y where the data in X has length different than 24.\n",
        "\n",
        "    Parameters:\n",
        "    - X: List of 2D numpy arrays, where each 2D array has shape (1, N).\n",
        "    - y: Numpy array of labels, where the ith label corresponds to the ith element in X.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_data_np: 2D numpy array containing only the data with length 24.\n",
        "    - filtered_labels_np: Numpy array containing the corresponding labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize empty lists for filtered data and labels\n",
        "    filtered_data = []\n",
        "    filtered_labels = []\n",
        "\n",
        "    # Loop through the data and labels and filter out elements where the data length is not 24\n",
        "    for i, arr in enumerate(X):\n",
        "        if arr.shape[1] == 24:\n",
        "            filtered_data.append(arr)\n",
        "            filtered_labels.append(y[i])\n",
        "\n",
        "    # Convert filtered data to a 2D NumPy array\n",
        "    filtered_data_np = np.concatenate(filtered_data, axis=0)\n",
        "\n",
        "    # Convert filtered labels to a NumPy array\n",
        "    filtered_labels_np = np.array(filtered_labels)\n",
        "\n",
        "    #if the number of unique values in y_clean is not equal to the number of unique values in y\n",
        "    #then there are missing values in the labels. Print it out\n",
        "    if len(np.unique(filtered_labels)) != len(np.unique(y)):\n",
        "        print(\"There are missing values in the labels\")\n",
        "\n",
        "    return filtered_data_np, filtered_labels_np\n",
        "\n",
        "\n",
        "def clean_nans(X, y):\n",
        "    \"\"\"\n",
        "    Removes time series with missing values and their corresponding labels.\n",
        "\n",
        "    Parameters:\n",
        "    - X: List of 2D numpy arrays, where each 2D array has shape (1, N).\n",
        "    - y: Numpy array of labels, where the ith label corresponds to the ith element in X.\n",
        "\n",
        "    Returns:\n",
        "    - X_clean: 2D numpy array containing only the data with length 24.\n",
        "    - y_clean: Numpy array containing the corresponding labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find which time series contain missing values\n",
        "    missing_values_indices = np.any(np.isnan(X), axis=1)\n",
        "\n",
        "    # Remove time series with missing values and their corresponding labels\n",
        "    X_clean = X[~missing_values_indices]\n",
        "    y_clean = y[~missing_values_indices]\n",
        "\n",
        "    #if the number of unique values in y_clean is not equal to the number of unique values in y\n",
        "    #then there are missing values in the labels. Print it out\n",
        "    if len(np.unique(y_clean)) != len(np.unique(y)):\n",
        "        print(\"There are missing values in the labels\")\n",
        "\n",
        "    return X_clean, y_clean\n",
        "\n",
        "\n",
        "def load_classification_with_fixes(dataset):\n",
        "    \"\"\"\n",
        "    Loads a classification dataset from the UCR archive and applies fixes to it if required\n",
        "    \"\"\"\n",
        "\n",
        "    if(dataset == 'MelbournePedestrian'):\n",
        "        X, y, _ = load_classification(dataset)\n",
        "        X, y = fix_MelbournePedestrian(X, y)\n",
        "        X, y = clean_nans(X, y)\n",
        "    elif(dataset in datasets_with_nans):\n",
        "        X, y, _ = load_classification(dataset)\n",
        "        X = np.squeeze(X)\n",
        "        X, y = clean_nans(X, y)\n",
        "    else:\n",
        "        X, y, _ = load_classification(dataset)\n",
        "        X = np.squeeze(X)\n",
        "\n",
        "    return X.astype(np.float32),y\n",
        "\n",
        "#Test load_classification_with_fixes in corresponding datasets\n",
        "X, y = load_classification_with_fixes('MelbournePedestrian')\n",
        "#print shape and dataset name\n",
        "print(\" Shape of X = \", X.shape)\n",
        "X, y = load_classification_with_fixes('DodgerLoopDay')\n",
        "print(\" Shape of X = \", X.shape)\n",
        "\n",
        "X, y = load_classification_with_fixes('DodgerLoopGame')\n",
        "print(\" Shape of X = \", X.shape)\n",
        "\n",
        "X, y = load_classification_with_fixes('DodgerLoopWeekend')\n",
        "print(\" Shape of X = \", X.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps7nLtVIjbd6",
        "outputId": "5a4fab58-625c-40a4-f6b2-f3afb02619a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (3457, 24)\n",
            " Shape of X =  (144, 288)\n",
            " Shape of X =  (144, 288)\n",
            " Shape of X =  (144, 288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "S9pkE4qVEdWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rA153rAdboOO"
      },
      "outputs": [],
      "source": [
        "#@title Modified feature extraction stage of Minirocket\n",
        "\n",
        "# Modified version of the Minirocket algorithm:\n",
        "# Angus Dempster, Daniel F Schmidt, Geoffrey I Webb\n",
        "\n",
        "# MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series\n",
        "# Classification\n",
        "\n",
        "# https://arxiv.org/abs/2012.08791\n",
        "\n",
        "########MODIFICATIONS############\n",
        "#1. We randomly permute the indices array used for choosing the kernel components (see array indices and compare with original)\n",
        "#2. We randomly permute the array of quantiles in the function called fit\n",
        "\n",
        "#These changes successfully eliminate artificial periodic patterns in the transformed input.\n",
        "\n",
        "\n",
        "from numba import njit, prange, vectorize\n",
        "\n",
        "@njit(\"float32[:](float32[:,:],int32[:],int32[:],float32[:])\", fastmath = True, parallel = False, cache = True)\n",
        "def _fit_biases(X, dilations, num_features_per_dilation, quantiles):\n",
        "\n",
        "    num_examples, input_length = X.shape\n",
        "\n",
        "    # equivalent to:\n",
        "    # >>> from itertools import combinations\n",
        "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
        "    ###MODIFICATION\n",
        "    indices = np.array((\n",
        "       1, 3, 6, 1, 2, 7, 1, 2, 3, 0, 2, 3, 1, 4, 5, 0, 1, 3, 3, 5, 6, 0,\n",
        "       1, 2, 2, 5, 8, 1, 3, 7, 0, 1, 8, 4, 6, 7, 0, 1, 4, 3, 4, 6, 0, 4,\n",
        "       5, 2, 6, 7, 5, 6, 7, 0, 1, 6, 4, 5, 7, 4, 7, 8, 1, 6, 8, 0, 2, 6,\n",
        "       5, 6, 8, 2, 5, 7, 0, 1, 7, 0, 7, 8, 0, 3, 5, 0, 3, 7, 2, 3, 8, 2,\n",
        "       3, 4, 1, 4, 6, 3, 4, 5, 0, 3, 8, 4, 5, 8, 0, 4, 6, 1, 4, 8, 6, 7,\n",
        "       8, 4, 6, 8, 0, 3, 4, 1, 3, 4, 1, 5, 7, 1, 4, 7, 1, 2, 8, 0, 6, 7,\n",
        "       1, 6, 7, 1, 3, 5, 0, 1, 5, 0, 4, 8, 4, 5, 6, 0, 2, 5, 3, 5, 7, 0,\n",
        "       2, 4, 2, 6, 8, 2, 3, 7, 2, 5, 6, 2, 4, 8, 0, 2, 7, 3, 6, 8, 2, 3,\n",
        "       6, 3, 7, 8, 0, 5, 8, 1, 2, 6, 2, 3, 5, 1, 5, 8, 3, 6, 7, 3, 4, 7,\n",
        "       0, 4, 7, 3, 5, 8, 2, 4, 5, 1, 2, 5, 2, 7, 8, 2, 4, 6, 0, 5, 6, 3,\n",
        "       4, 8, 0, 6, 8, 2, 4, 7, 0, 2, 8, 0, 3, 6, 5, 7, 8, 1, 5, 6, 1, 2,\n",
        "       4, 0, 5, 7, 1, 3, 8, 1, 7, 8\n",
        "    ), dtype = np.int32).reshape(84, 3)\n",
        "\n",
        "    num_kernels = len(indices)\n",
        "    num_dilations = len(dilations)\n",
        "\n",
        "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
        "\n",
        "    biases = np.zeros(num_features, dtype = np.float32)\n",
        "\n",
        "    feature_index_start = 0\n",
        "\n",
        "    for dilation_index in range(num_dilations):\n",
        "\n",
        "        dilation = dilations[dilation_index]\n",
        "        padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "        num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
        "\n",
        "        for kernel_index in range(num_kernels):\n",
        "\n",
        "            feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "            _X = X[np.random.randint(num_examples)]\n",
        "\n",
        "            A = -_X          # A = alpha * X = -X\n",
        "            G = _X + _X + _X # G = gamma * X = 3X\n",
        "\n",
        "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
        "            C_alpha[:] = A\n",
        "\n",
        "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
        "            C_gamma[9 // 2] = G\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "\n",
        "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
        "                C_gamma[gamma_index, -end:] = G[:end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "\n",
        "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
        "                C_gamma[gamma_index, :-start] = G[start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "            C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
        "\n",
        "            biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[feature_index_start:feature_index_end])\n",
        "\n",
        "            feature_index_start = feature_index_end\n",
        "\n",
        "    return biases\n",
        "\n",
        "def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
        "\n",
        "    num_kernels = 84\n",
        "\n",
        "    num_features_per_kernel = num_features // num_kernels\n",
        "    true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
        "    multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
        "\n",
        "    max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
        "    dilations, num_features_per_dilation = \\\n",
        "    np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base = 2).astype(np.int32), return_counts = True)\n",
        "    num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32) # this is a vector\n",
        "\n",
        "    remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
        "    i = 0\n",
        "    while remainder > 0:\n",
        "        num_features_per_dilation[i] += 1\n",
        "        remainder -= 1\n",
        "        i = (i + 1) % len(num_features_per_dilation)\n",
        "\n",
        "    return dilations, num_features_per_dilation\n",
        "\n",
        "# low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
        "def _quantiles(n):\n",
        "    return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype = np.float32)\n",
        "\n",
        "def fit(X, num_features = 10_000, max_dilations_per_kernel = 32):\n",
        "\n",
        "    _, input_length = X.shape\n",
        "\n",
        "    num_kernels = 84\n",
        "\n",
        "    dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
        "\n",
        "    num_features_per_kernel = np.sum(num_features_per_dilation)\n",
        "\n",
        "    quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
        "\n",
        "    ###MODIFICATION\n",
        "    quantiles = np.random.permutation(quantiles)\n",
        "\n",
        "    biases = _fit_biases(X, dilations, num_features_per_dilation, quantiles)\n",
        "\n",
        "\n",
        "    return dilations, num_features_per_dilation, biases\n",
        "\n",
        "# _PPV(C, b).mean() returns PPV for vector C (convolution output) and scalar b (bias)\n",
        "@vectorize(\"float32(float32,float32)\", nopython = True, cache = True)\n",
        "def _PPV(a, b):\n",
        "    if a > b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "@njit(\"float32[:,:](float32[:,:],Tuple((int32[:],int32[:],float32[:])))\", fastmath = True, parallel = True, cache = True)\n",
        "def transform(X, parameters):\n",
        "\n",
        "    num_examples, input_length = X.shape\n",
        "\n",
        "    dilations, num_features_per_dilation, biases = parameters\n",
        "\n",
        "    # equivalent to:\n",
        "    # >>> from itertools import combinations\n",
        "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
        "    ###MODIFICATION\n",
        "    indices = np.array((\n",
        "       1, 3, 6, 1, 2, 7, 1, 2, 3, 0, 2, 3, 1, 4, 5, 0, 1, 3, 3, 5, 6, 0,\n",
        "       1, 2, 2, 5, 8, 1, 3, 7, 0, 1, 8, 4, 6, 7, 0, 1, 4, 3, 4, 6, 0, 4,\n",
        "       5, 2, 6, 7, 5, 6, 7, 0, 1, 6, 4, 5, 7, 4, 7, 8, 1, 6, 8, 0, 2, 6,\n",
        "       5, 6, 8, 2, 5, 7, 0, 1, 7, 0, 7, 8, 0, 3, 5, 0, 3, 7, 2, 3, 8, 2,\n",
        "       3, 4, 1, 4, 6, 3, 4, 5, 0, 3, 8, 4, 5, 8, 0, 4, 6, 1, 4, 8, 6, 7,\n",
        "       8, 4, 6, 8, 0, 3, 4, 1, 3, 4, 1, 5, 7, 1, 4, 7, 1, 2, 8, 0, 6, 7,\n",
        "       1, 6, 7, 1, 3, 5, 0, 1, 5, 0, 4, 8, 4, 5, 6, 0, 2, 5, 3, 5, 7, 0,\n",
        "       2, 4, 2, 6, 8, 2, 3, 7, 2, 5, 6, 2, 4, 8, 0, 2, 7, 3, 6, 8, 2, 3,\n",
        "       6, 3, 7, 8, 0, 5, 8, 1, 2, 6, 2, 3, 5, 1, 5, 8, 3, 6, 7, 3, 4, 7,\n",
        "       0, 4, 7, 3, 5, 8, 2, 4, 5, 1, 2, 5, 2, 7, 8, 2, 4, 6, 0, 5, 6, 3,\n",
        "       4, 8, 0, 6, 8, 2, 4, 7, 0, 2, 8, 0, 3, 6, 5, 7, 8, 1, 5, 6, 1, 2,\n",
        "       4, 0, 5, 7, 1, 3, 8, 1, 7, 8\n",
        "    ), dtype = np.int32).reshape(84, 3)\n",
        "\n",
        "\n",
        "    num_kernels = len(indices)\n",
        "    num_dilations = len(dilations)\n",
        "\n",
        "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
        "\n",
        "    features = np.zeros((num_examples, num_features), dtype = np.float32)\n",
        "\n",
        "    for example_index in prange(num_examples):\n",
        "\n",
        "        _X = X[example_index]\n",
        "\n",
        "        A = -_X          # A = alpha * X = -X\n",
        "        G = _X + _X + _X # G = gamma * X = 3X\n",
        "\n",
        "        feature_index_start = 0\n",
        "\n",
        "        for dilation_index in range(num_dilations):\n",
        "\n",
        "            _padding0 = dilation_index % 2\n",
        "\n",
        "            dilation = dilations[dilation_index]\n",
        "            padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "            num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
        "\n",
        "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
        "            C_alpha[:] = A\n",
        "\n",
        "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
        "            C_gamma[9 // 2] = G\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "\n",
        "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
        "                C_gamma[gamma_index, -end:] = G[:end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "\n",
        "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
        "                C_gamma[gamma_index, :-start] = G[start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            for kernel_index in range(num_kernels):\n",
        "\n",
        "                feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "                _padding1 = (_padding0 + kernel_index) % 2\n",
        "\n",
        "                index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "                C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
        "\n",
        "                if _padding1 == 0:\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        features[example_index, feature_index_start + feature_count] = _PPV(C, biases[feature_index_start + feature_count]).mean()\n",
        "                else:\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        features[example_index, feature_index_start + feature_count] = _PPV(C[padding:-padding], biases[feature_index_start + feature_count]).mean()\n",
        "\n",
        "                feature_index_start = feature_index_end\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbDH5XCkdido"
      },
      "source": [
        "#EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title R-Clustering evaluated across UCR dataset\n",
        "\n",
        "# Define the number of features or kernels (500 by default based on previous development experiments)\n",
        "num_features = 500\n",
        "\n",
        "# Define the destination to save results, default is the current folder, but it can be modified\n",
        "destination = \"Results_all.xlsx\"\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results_list = []\n",
        "i = 0\n",
        "\n",
        "# Iterate over the validation datasets\n",
        "# for dataset in validation_datasets:\n",
        "for dataset in all_valid_datasets:\n",
        "\n",
        "    print(dataset)\n",
        "\n",
        "    # Convert the sktime datasets to numpy arrays\n",
        "    X, Y = load_classification_with_fixes(dataset)\n",
        "\n",
        "\n",
        "    #######################################################################################\n",
        "    # STAGE 1: DATA TRANSFORMATION WITH THE MODIFIED MINIROCKET ALGORITHM\n",
        "    #######################################################################################\n",
        "    # Fit and transform data according to the modified minirocket model with the given number of features\n",
        "    parameters = fit(X=X, num_features=num_features)\n",
        "    transformed_data = transform(X=X, parameters=parameters)\n",
        "\n",
        "    #######################################################################################\n",
        "    # STAGE 2: DIMENSIONALITY REDUCTION WITH PCA\n",
        "    #######################################################################################\n",
        "    # Standardize the data to have mean=0 and variance=1 for better performance of machine learning models\n",
        "    sc = StandardScaler()\n",
        "    X_std = sc.fit_transform(transformed_data)\n",
        "\n",
        "    # Implement Principal Component Analysis (PCA) to reduce dimensions and focus on relevant features\n",
        "    pca = PCA().fit(X_std)\n",
        "\n",
        "    # Find the optimal number of dimensions based on the criterion that the explained variance ratio is less than 0.01\n",
        "    optimal_dimensions = np.argmax(pca.explained_variance_ratio_ < 0.01)\n",
        "\n",
        "    # Apply PCA with the optimal number of dimensions\n",
        "    pca_optimal = PCA(n_components=optimal_dimensions)\n",
        "    transformed_data_pca = pca_optimal.fit_transform(X_std)\n",
        "    print(\"Dimensions: \", transformed_data_pca.shape[1])\n",
        "\n",
        "    #######################################################################################\n",
        "    # STAGE 3: CLUSTERING ALGORITHM: K-Means\n",
        "    #######################################################################################\n",
        "    # Get the number of clusters of the dataset\n",
        "    num_clusters = len(np.unique(Y))\n",
        "    # Apply KMeans clustering on the transformed data\n",
        "    labels_pred = KMeans(n_clusters=num_clusters, n_init=10).fit_predict(transformed_data_pca)\n",
        "\n",
        "\n",
        "    #######################################################################################\n",
        "    # EVALUATION\n",
        "    #######################################################################################\n",
        "\n",
        "    # Evaluate the clustering using Adjusted Rand Index (ARI) score\n",
        "    score = metrics.adjusted_rand_score(labels_true=Y, labels_pred=labels_pred)\n",
        "    print(\"ARI: \", score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Store each result in a dictionary to create a pandas dataframe later\n",
        "    results_row = dict(Dataset=dataset, ari_score=score)\n",
        "    results_list.append(results_row)\n",
        "\n",
        "    # Save interim results every 5 iterations\n",
        "    i += 1\n",
        "    if i % 5 == 0:\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "        results_df.to_excel(destination)\n",
        "\n",
        "# Convert the results list to a pandas DataFrame and save to an Excel file\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df.to_excel(destination)\n"
      ],
      "metadata": {
        "id": "nX9amC-WHPuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QVIVk2IIYZey",
        "S9pkE4qVEdWx"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1NkEml1BXwFljRqLB-h1oWyPxCQ9fvRij",
      "authorship_tag": "ABX9TyM1PZDiInJqRz70vZllo29T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}