{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgemarcoes/R-Clustering/blob/main/R_Clustering_on_UCR_Archive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIVk2IIYZey"
      },
      "source": [
        "#LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0QW1ScqC4Tq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comment if you do not need to install the sktime package\n",
        "!pip install sktime\n",
        "from sktime.datasets import load_UCR_UEA_dataset"
      ],
      "metadata": {
        "id": "ZxKjtCmTDQRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQmmDvWLjeHD"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UCR ARCHIVE"
      ],
      "metadata": {
        "id": "lWBiTZ2RzI_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXaGGLQWc5oy"
      },
      "outputs": [],
      "source": [
        "#@title Datasets\n",
        "benchmark_datasets = ['ACSF1', 'Adiac', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken',\n",
        "'BME', 'Car', 'CBF', 'Chinatown', 'ChlorineConcentration',\n",
        "'CinCECGTorso', 'Coffee', 'Computers', 'CricketX', 'CricketY',\n",
        "'CricketZ', 'Crop', 'DiatomSizeReduction',\n",
        "'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect',\n",
        "'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame',\n",
        "'DodgerLoopWeekend', 'Earthquakes', 'ECG5000', 'ECGFiveDays',\n",
        "'ElectricDevices', 'EOGHorizontalSignal', 'EOGVerticalSignal',\n",
        "'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR', 'FiftyWords',\n",
        "'Fish', 'FreezerRegularTrain', 'FreezerSmallTrain', 'Fungi',\n",
        "'GunPoint', 'GunPointAgeSpan', 'GunPointMaleVersusFemale',\n",
        "'GunPointOldVersusYoung', 'Ham', 'HandOutlines', 'Haptics',\n",
        "'Herring', 'HouseTwenty', 'InlineSkate', 'InsectEPGRegularTrain',\n",
        "'InsectEPGSmallTrain', 'InsectWingbeatSound', 'ItalyPowerDemand',\n",
        "'LargeKitchenAppliances', 'Lightning7', 'Mallat', 'Meat',\n",
        "'MedicalImages', 'MelbournePedestrian',\n",
        "'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect',\n",
        "'MiddlePhalanxTW', 'MixedShapesRegularTrain',\n",
        "'MixedShapesSmallTrain', 'MoteStrain',\n",
        "'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2',\n",
        "'OliveOil', 'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme',\n",
        "'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'Plane',\n",
        "'PowerCons', 'ProximalPhalanxOutlineAgeGroup',\n",
        "'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW',\n",
        "'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2',\n",
        "'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShapeletSim',\n",
        "'ShapesAll', 'SmallKitchenAppliances', 'SmoothSubspace',\n",
        "'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2',\n",
        "'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols',\n",
        "'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2',\n",
        "'Trace', 'TwoLeadECG', 'TwoPatterns', 'UMD',\n",
        "'UWaveGestureLibraryAll', 'UWaveGestureLibraryX',\n",
        "'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wine',\n",
        "'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga']\n",
        "\n",
        "rocket_development_datasets = ['Beef', 'BirdChicken',\n",
        "'Car', 'CricketX','CricketY','CricketZ','DistalPhalanxTW','ECG5000',\n",
        "'FiftyWords','Fish','Haptics','Herring','InsectWingbeatSound','ItalyPowerDemand',\n",
        "'LargeKitchenAppliances','Lightning7','Meat','MedicalImages',\n",
        "'MiddlePhalanxOutlineAgeGroup','OSULeaf','OliveOil','Phoneme',\n",
        "'Plane','ProximalPhalanxOutlineCorrect','ProximalPhalanxTW','ScreenType','ShapeletSim',\n",
        "'Strawberry','SwedishLeaf','SyntheticControl','ToeSegmentation1','Trace','UWaveGestureLibraryY',\n",
        "'WordSynonyms','Worms','Yoga']\n",
        "\n",
        "validation_datasets = np.setdiff1d(benchmark_datasets, rocket_development_datasets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sktime_to_numpy(dataset_name):\n",
        "  \"\"\"\n",
        "   This function retrieves a dataset from the UCR archive using the sktime library,\n",
        "   including both the train and test sets, and converts it to a numpy format.\n",
        "   It also addresses an issue with the Melbourne Pedestrian dataset\n",
        "  \"\"\"\n",
        "  X, y = load_UCR_UEA_dataset(name=dataset_name)\n",
        "  if(dataset_name == 'MelbournePedestrian'): #Missing data in this dataser\n",
        "    X.drop(3030,inplace=True)\n",
        "    X.drop(3304,inplace=True)\n",
        "    y = np.delete(y,3030)\n",
        "    y = np.delete(y,3304)\n",
        "  X_numpy = []\n",
        "  for index, row in X.iterrows():\n",
        "    X_numpy.append(row.dim_0.to_numpy())\n",
        "  X_numpy_ = np.array(X_numpy)\n",
        "  return X_numpy_.astype(np.float32),y"
      ],
      "metadata": {
        "id": "MJSst8afE3WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "S9pkE4qVEdWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA153rAdboOO"
      },
      "outputs": [],
      "source": [
        "#@title Modified feature extraction stage of Minirocket\n",
        "\n",
        "# Modified version of the Minirocket algorithm:\n",
        "# Angus Dempster, Daniel F Schmidt, Geoffrey I Webb\n",
        "\n",
        "# MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series\n",
        "# Classification\n",
        "\n",
        "# https://arxiv.org/abs/2012.08791\n",
        "\n",
        "########MODIFICATIONS############\n",
        "#1. We randomly permute the indices array used for choosing the kernel components (see array indices and compare with original)\n",
        "#2. We randomly permute the array of quantiles in the function called fit\n",
        "\n",
        "#These changes successfully eliminate artificial periodic patterns in the transformed input.\n",
        "\n",
        "\n",
        "from numba import njit, prange, vectorize\n",
        "\n",
        "@njit(\"float32[:](float32[:,:],int32[:],int32[:],float32[:])\", fastmath = True, parallel = False, cache = True)\n",
        "def _fit_biases(X, dilations, num_features_per_dilation, quantiles):\n",
        "\n",
        "    num_examples, input_length = X.shape\n",
        "\n",
        "    # equivalent to:\n",
        "    # >>> from itertools import combinations\n",
        "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
        "    ###MODIFICATION\n",
        "    indices = np.array((\n",
        "       1, 3, 6, 1, 2, 7, 1, 2, 3, 0, 2, 3, 1, 4, 5, 0, 1, 3, 3, 5, 6, 0,\n",
        "       1, 2, 2, 5, 8, 1, 3, 7, 0, 1, 8, 4, 6, 7, 0, 1, 4, 3, 4, 6, 0, 4,\n",
        "       5, 2, 6, 7, 5, 6, 7, 0, 1, 6, 4, 5, 7, 4, 7, 8, 1, 6, 8, 0, 2, 6,\n",
        "       5, 6, 8, 2, 5, 7, 0, 1, 7, 0, 7, 8, 0, 3, 5, 0, 3, 7, 2, 3, 8, 2,\n",
        "       3, 4, 1, 4, 6, 3, 4, 5, 0, 3, 8, 4, 5, 8, 0, 4, 6, 1, 4, 8, 6, 7,\n",
        "       8, 4, 6, 8, 0, 3, 4, 1, 3, 4, 1, 5, 7, 1, 4, 7, 1, 2, 8, 0, 6, 7,\n",
        "       1, 6, 7, 1, 3, 5, 0, 1, 5, 0, 4, 8, 4, 5, 6, 0, 2, 5, 3, 5, 7, 0,\n",
        "       2, 4, 2, 6, 8, 2, 3, 7, 2, 5, 6, 2, 4, 8, 0, 2, 7, 3, 6, 8, 2, 3,\n",
        "       6, 3, 7, 8, 0, 5, 8, 1, 2, 6, 2, 3, 5, 1, 5, 8, 3, 6, 7, 3, 4, 7,\n",
        "       0, 4, 7, 3, 5, 8, 2, 4, 5, 1, 2, 5, 2, 7, 8, 2, 4, 6, 0, 5, 6, 3,\n",
        "       4, 8, 0, 6, 8, 2, 4, 7, 0, 2, 8, 0, 3, 6, 5, 7, 8, 1, 5, 6, 1, 2,\n",
        "       4, 0, 5, 7, 1, 3, 8, 1, 7, 8\n",
        "    ), dtype = np.int32).reshape(84, 3)\n",
        "\n",
        "    num_kernels = len(indices)\n",
        "    num_dilations = len(dilations)\n",
        "\n",
        "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
        "\n",
        "    biases = np.zeros(num_features, dtype = np.float32)\n",
        "\n",
        "    feature_index_start = 0\n",
        "\n",
        "    for dilation_index in range(num_dilations):\n",
        "\n",
        "        dilation = dilations[dilation_index]\n",
        "        padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "        num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
        "\n",
        "        for kernel_index in range(num_kernels):\n",
        "\n",
        "            feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "            _X = X[np.random.randint(num_examples)]\n",
        "\n",
        "            A = -_X          # A = alpha * X = -X\n",
        "            G = _X + _X + _X # G = gamma * X = 3X\n",
        "\n",
        "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
        "            C_alpha[:] = A\n",
        "\n",
        "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
        "            C_gamma[9 // 2] = G\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "\n",
        "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
        "                C_gamma[gamma_index, -end:] = G[:end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "\n",
        "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
        "                C_gamma[gamma_index, :-start] = G[start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "            C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
        "\n",
        "            biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[feature_index_start:feature_index_end])\n",
        "\n",
        "            feature_index_start = feature_index_end\n",
        "\n",
        "    return biases\n",
        "\n",
        "def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
        "\n",
        "    num_kernels = 84\n",
        "\n",
        "    num_features_per_kernel = num_features // num_kernels\n",
        "    true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
        "    multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
        "\n",
        "    max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
        "    dilations, num_features_per_dilation = \\\n",
        "    np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base = 2).astype(np.int32), return_counts = True)\n",
        "    num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32) # this is a vector\n",
        "\n",
        "    remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
        "    i = 0\n",
        "    while remainder > 0:\n",
        "        num_features_per_dilation[i] += 1\n",
        "        remainder -= 1\n",
        "        i = (i + 1) % len(num_features_per_dilation)\n",
        "\n",
        "    return dilations, num_features_per_dilation\n",
        "\n",
        "# low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
        "def _quantiles(n):\n",
        "    return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype = np.float32)\n",
        "\n",
        "def fit(X, num_features = 10_000, max_dilations_per_kernel = 32):\n",
        "\n",
        "    _, input_length = X.shape\n",
        "\n",
        "    num_kernels = 84\n",
        "\n",
        "    dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
        "\n",
        "    num_features_per_kernel = np.sum(num_features_per_dilation)\n",
        "\n",
        "    quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
        "\n",
        "    ###MODIFICATION\n",
        "    quantiles = np.random.permutation(quantiles)\n",
        "\n",
        "    biases = _fit_biases(X, dilations, num_features_per_dilation, quantiles)\n",
        "\n",
        "\n",
        "    return dilations, num_features_per_dilation, biases\n",
        "\n",
        "# _PPV(C, b).mean() returns PPV for vector C (convolution output) and scalar b (bias)\n",
        "@vectorize(\"float32(float32,float32)\", nopython = True, cache = True)\n",
        "def _PPV(a, b):\n",
        "    if a > b:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "@njit(\"float32[:,:](float32[:,:],Tuple((int32[:],int32[:],float32[:])))\", fastmath = True, parallel = True, cache = True)\n",
        "def transform(X, parameters):\n",
        "\n",
        "    num_examples, input_length = X.shape\n",
        "\n",
        "    #LOS BIASES LOS HEMOS TRANSFORMADO A TRAVÉS DE LOS QUANTILES\n",
        "    dilations, num_features_per_dilation, biases = parameters\n",
        "\n",
        "    # equivalent to:\n",
        "    # >>> from itertools import combinations\n",
        "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
        "    ###MODIFICATION\n",
        "    indices = np.array((\n",
        "       1, 3, 6, 1, 2, 7, 1, 2, 3, 0, 2, 3, 1, 4, 5, 0, 1, 3, 3, 5, 6, 0,\n",
        "       1, 2, 2, 5, 8, 1, 3, 7, 0, 1, 8, 4, 6, 7, 0, 1, 4, 3, 4, 6, 0, 4,\n",
        "       5, 2, 6, 7, 5, 6, 7, 0, 1, 6, 4, 5, 7, 4, 7, 8, 1, 6, 8, 0, 2, 6,\n",
        "       5, 6, 8, 2, 5, 7, 0, 1, 7, 0, 7, 8, 0, 3, 5, 0, 3, 7, 2, 3, 8, 2,\n",
        "       3, 4, 1, 4, 6, 3, 4, 5, 0, 3, 8, 4, 5, 8, 0, 4, 6, 1, 4, 8, 6, 7,\n",
        "       8, 4, 6, 8, 0, 3, 4, 1, 3, 4, 1, 5, 7, 1, 4, 7, 1, 2, 8, 0, 6, 7,\n",
        "       1, 6, 7, 1, 3, 5, 0, 1, 5, 0, 4, 8, 4, 5, 6, 0, 2, 5, 3, 5, 7, 0,\n",
        "       2, 4, 2, 6, 8, 2, 3, 7, 2, 5, 6, 2, 4, 8, 0, 2, 7, 3, 6, 8, 2, 3,\n",
        "       6, 3, 7, 8, 0, 5, 8, 1, 2, 6, 2, 3, 5, 1, 5, 8, 3, 6, 7, 3, 4, 7,\n",
        "       0, 4, 7, 3, 5, 8, 2, 4, 5, 1, 2, 5, 2, 7, 8, 2, 4, 6, 0, 5, 6, 3,\n",
        "       4, 8, 0, 6, 8, 2, 4, 7, 0, 2, 8, 0, 3, 6, 5, 7, 8, 1, 5, 6, 1, 2,\n",
        "       4, 0, 5, 7, 1, 3, 8, 1, 7, 8\n",
        "    ), dtype = np.int32).reshape(84, 3)\n",
        "\n",
        "\n",
        "    num_kernels = len(indices)\n",
        "    num_dilations = len(dilations)\n",
        "\n",
        "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
        "\n",
        "    features = np.zeros((num_examples, num_features), dtype = np.float32)\n",
        "\n",
        "    for example_index in prange(num_examples):\n",
        "\n",
        "        _X = X[example_index]\n",
        "\n",
        "        A = -_X          # A = alpha * X = -X\n",
        "        G = _X + _X + _X # G = gamma * X = 3X\n",
        "\n",
        "        feature_index_start = 0\n",
        "\n",
        "        for dilation_index in range(num_dilations):\n",
        "\n",
        "            _padding0 = dilation_index % 2\n",
        "\n",
        "            dilation = dilations[dilation_index]\n",
        "            padding = ((9 - 1) * dilation) // 2\n",
        "\n",
        "            num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
        "\n",
        "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
        "            C_alpha[:] = A\n",
        "\n",
        "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
        "            C_gamma[9 // 2] = G\n",
        "\n",
        "            start = dilation\n",
        "            end = input_length - padding\n",
        "\n",
        "            for gamma_index in range(9 // 2):\n",
        "\n",
        "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
        "                C_gamma[gamma_index, -end:] = G[:end]\n",
        "\n",
        "                end += dilation\n",
        "\n",
        "            for gamma_index in range(9 // 2 + 1, 9):\n",
        "\n",
        "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
        "                C_gamma[gamma_index, :-start] = G[start:]\n",
        "\n",
        "                start += dilation\n",
        "\n",
        "            for kernel_index in range(num_kernels):\n",
        "\n",
        "                feature_index_end = feature_index_start + num_features_this_dilation\n",
        "\n",
        "                _padding1 = (_padding0 + kernel_index) % 2\n",
        "\n",
        "                index_0, index_1, index_2 = indices[kernel_index]\n",
        "\n",
        "                C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
        "\n",
        "                if _padding1 == 0:\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        features[example_index, feature_index_start + feature_count] = _PPV(C, biases[feature_index_start + feature_count]).mean()\n",
        "                else:\n",
        "                    for feature_count in range(num_features_this_dilation):\n",
        "                        features[example_index, feature_index_start + feature_count] = _PPV(C[padding:-padding], biases[feature_index_start + feature_count]).mean()\n",
        "\n",
        "                feature_index_start = feature_index_end\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbDH5XCkdido"
      },
      "source": [
        "#EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title R-Clustering evaluated across UCR dataset\n",
        "\n",
        "# Define the number of features or kernels (500 by default based on previous development experiments)\n",
        "num_features = 500\n",
        "\n",
        "# Define the destination to save results, default is the current folder, but it can be modified\n",
        "destination = \"Results.xlsx\"\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results_list = []\n",
        "i = 0\n",
        "\n",
        "# Iterate over the validation datasets\n",
        "for dataset in validation_datasets:\n",
        "    print(dataset)\n",
        "\n",
        "    # Convert the sktime datasets to numpy arrays\n",
        "    X, Y = sktime_to_numpy(dataset)\n",
        "\n",
        "\n",
        "    #######################################################################################\n",
        "    # STAGE 1: DATA TRANSFORMATION WITH THE MODIFIED MINIROCKET ALGORITHM\n",
        "    #######################################################################################\n",
        "    # Fit and transform data according to the modified minirocket model with the given number of features\n",
        "    parameters = fit(X=X, num_features=num_features)\n",
        "    transformed_data = transform(X=X, parameters=parameters)\n",
        "\n",
        "    #######################################################################################\n",
        "    # STAGE 2: DIMENSIONALITY REDUCTION WITH PCA\n",
        "    #######################################################################################\n",
        "    # Standardize the data to have mean=0 and variance=1 for better performance of machine learning models\n",
        "    sc = StandardScaler()\n",
        "    X_std = sc.fit_transform(transformed_data)\n",
        "\n",
        "    # Implement Principal Component Analysis (PCA) to reduce dimensions and focus on relevant features\n",
        "    pca = PCA().fit(X_std)\n",
        "\n",
        "    # Find the optimal number of dimensions based on the criterion that the explained variance ratio is less than 0.01\n",
        "    optimal_dimensions = np.argmax(pca.explained_variance_ratio_ < 0.01)\n",
        "\n",
        "    # Apply PCA with the optimal number of dimensions\n",
        "    pca_optimal = PCA(n_components=optimal_dimensions)\n",
        "    transformed_data_pca = pca_optimal.fit_transform(X_std)\n",
        "    print(\"Dimensions: \", transformed_data_pca.shape[1])\n",
        "\n",
        "    #######################################################################################\n",
        "    # STAGE 3: CLUSTERING ALGORITHM: K-Means\n",
        "    #######################################################################################\n",
        "    # Get the number of clusters of the dataset\n",
        "    num_clusters = len(np.unique(Y))\n",
        "    # Apply KMeans clustering on the transformed data\n",
        "    labels_pred = KMeans(n_clusters=num_clusters, n_init=10).fit_predict(transformed_data_pca)\n",
        "\n",
        "\n",
        "    #######################################################################################\n",
        "    # EVALUATION\n",
        "    #######################################################################################\n",
        "\n",
        "    # Evaluate the clustering using Adjusted Rand Index (ARI) score\n",
        "    score = metrics.adjusted_rand_score(labels_true=Y, labels_pred=labels_pred)\n",
        "    print(\"ARI: \", score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Store each result in a dictionary to create a pandas dataframe later\n",
        "    results_row = dict(Dataset=dataset, ari_score=score)\n",
        "    results_list.append(results_row)\n",
        "\n",
        "    # Save interim results every 5 iterations\n",
        "    i += 1\n",
        "    if i % 5 == 0:\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "        results_df.to_excel(destination)\n",
        "\n",
        "# Convert the results list to a pandas DataFrame and save to an Excel file\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df.to_excel(destination)\n"
      ],
      "metadata": {
        "id": "nX9amC-WHPuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QVIVk2IIYZey",
        "S9pkE4qVEdWx"
      ],
      "toc_visible": true,
      "mount_file_id": "1NkEml1BXwFljRqLB-h1oWyPxCQ9fvRij",
      "authorship_tag": "ABX9TyOXTvG6yV7JE1Q0Voqa0jl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}